{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mall\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m untar_data(URLs\u001b[38;5;241m.\u001b[39mPETS)\n\u001b[0;32m      5\u001b[0m additional_images \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fastai'"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "path = untar_data(URLs.PETS)\n",
    "\n",
    "additional_images = Path('images')\n",
    "ims = get_image_files(additional_images)\n",
    "for i, img in enumerate(ims):\n",
    "    category = img.parent.name\n",
    "    try:\n",
    "        im = PILImage.create(img)\n",
    "    except:\n",
    "        print(f\"Failed to open {img}\")\n",
    "        continue\n",
    "\n",
    "    shutil.copy(src=img, dst=path/'images'/f\"{category}_{i}.jpg\")\n",
    "\n",
    "failed = verify_images(path/'images')\n",
    "print(f\"Failed images: {failed}\")\n",
    "failed.map(Path.unlink)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dls = ImageDataLoaders.from_name_re(\n",
    "    path, \n",
    "    get_image_files(path/'images'), \n",
    "    pat='(.+)_\\d+.jpg', \n",
    "    item_tfms=RandomResizedCrop(460, min_scale=0.5),\n",
    "    # item_tfms=Resize(460), \n",
    "    batch_tfms=aug_transforms(size=224, min_scale=0.75))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learn = vision_learner(\n",
    "    dls, \n",
    "    models.resnet50, \n",
    "    metrics=accuracy)\n",
    "\n",
    "print(learn.dls.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learn.lr_find()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from fastai.vision.all import *\n",
    "import timm\n",
    "from fastai.metrics import accuracy\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to search\n",
    "    # img_size = trial.suggest_categorical('img_size', [224, 256, 384])\n",
    "    img_size = trial.suggest_categorical('img_size', [224, 256, 384, 512])\n",
    "    # models = ['convnext_tiny', 'resnet50', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3']\n",
    "    models = ['convnext_tiny','convnext_large', 'resnet50', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3']\n",
    "    selected_model = trial.suggest_categorical('model', models)\n",
    "    resize_method = trial.suggest_categorical('resize_method', ['Resize']) #, 'RandomResizedCrop', 'ResizeSquish', 'ResizeCrop'])\n",
    "    batch_transforms = trial.suggest_categorical('batch_transforms', [True])\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.2)\n",
    "    epochs = trial.suggest_int('epochs', 3, 10)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 0.002, 0.03, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam'])\n",
    "    \n",
    "    # Print the parameters for this trial\n",
    "    print(f\"Trial parameters: {trial.params}\")\n",
    "    \n",
    "    # Define transformations based on suggested hyperparameters\n",
    "    if resize_method == 'Resize':\n",
    "        item_tfms = [Resize(img_size)]\n",
    "    elif resize_method == 'RandomResizedCrop':\n",
    "        item_tfms = [RandomResizedCrop(img_size, min_scale=0.9)]\n",
    "    elif resize_method == 'ResizeSquish':\n",
    "        item_tfms = [Resize(img_size, method='squish')]\n",
    "    else:\n",
    "        item_tfms = [Resize(img_size, method='crop')]\n",
    "    \n",
    "    batch_tfms = [\n",
    "        *aug_transforms(  # Apply augmentations cautiously\n",
    "            size=img_size,\n",
    "            flip_vert=False,  # Avoid flipping vertically, important for medical data\n",
    "            max_rotate=30,    # Limit rotations to Â±10 degrees\n",
    "            max_zoom=1.5,     # Slight zooming\n",
    "            max_lighting=0.4, # Light contrast/brightness adjustment\n",
    "            p_affine=0.5,     # Probability of affine transform\n",
    "            p_lighting=0.3    # Probability of lighting transform\n",
    "        ),\n",
    "        Normalize.from_stats(*imagenet_stats)  # Normalize based on ImageNet mean/std\n",
    "    ] if batch_transforms else [Normalize.from_stats(*imagenet_stats)]\n",
    "\n",
    "    # Create DataLoaders\n",
    "    dls = ImageDataLoaders.from_name_re(\n",
    "        path, \n",
    "        get_image_files(path/'images'), \n",
    "        pat='(.+)_\\d+.jpg', \n",
    "        item_tfms=item_tfms,\n",
    "        batch_tfms=batch_tfms,\n",
    "        bs=batch_size,\n",
    "        shuffle_train=True)  # Shuffle the training data\n",
    "\n",
    "    # Check if the DataLoader is loaded correctly\n",
    "    dls.show_batch(max_n=4, figsize=(8, 8))\n",
    "\n",
    "    the_model = timm.create_model(selected_model, pretrained=True, num_classes=dls.c)\n",
    "    # Define the model with dropout\n",
    "    def create_model(the_model=the_model, dropout=0.5):\n",
    "        # Get the number of features from the classifier layer\n",
    "        num_ftrs = the_model.get_classifier().in_features\n",
    "        \n",
    "        # Replace the classifier with dropout and a linear layer\n",
    "        # The classifier layer is replaced regardless of its internal name\n",
    "        the_model.reset_classifier(num_classes=dls.c)  # Resets the classifier dynamically\n",
    "        the_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),  # Add dropout with the rate defined in your hyperparameter search\n",
    "            nn.Linear(num_ftrs, dls.c)\n",
    "        )\n",
    "        \n",
    "        return the_model\n",
    "    \n",
    "    # Create the learner\n",
    "    patience = 3\n",
    "    learn = Learner(\n",
    "        dls, \n",
    "        create_model(the_model, dropout), \n",
    "        metrics=accuracy,\n",
    "        wd=weight_decay,\n",
    "        # cbs=[EarlyStoppingCallback(monitor='accuracy', patience=patience)]\n",
    "        ).to_bf16()\n",
    "    \n",
    "    learn.lr_find(suggest_funcs=(slide, valley))\n",
    "    \n",
    "    # Choose optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        learn.opt_func = Adam\n",
    "    elif optimizer_name == 'SGD':\n",
    "        learn.opt_func = SGD\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        learn.opt_func = RMSProp\n",
    "    \n",
    "    # Train the model\n",
    "    learn.fit(epochs, lr)\n",
    "    \n",
    "    # Initialize accuracy\n",
    "    accuracy_result = 0.0\n",
    " \n",
    "    # Evaluate the model\n",
    "    try:\n",
    "        # Evaluate the model\n",
    "        validation_results = learn.validate()\n",
    "        if validation_results and len(validation_results) > 1:\n",
    "            accuracy_result = validation_results[1]\n",
    "    except Exception as e:\n",
    "        print(f\"Validation failed: {e}\")\n",
    "        accuracy_result = 0.0  # Default value if validation fails\n",
    " \n",
    "    return accuracy_result\n",
    "\n",
    "# Create a study and optimize the objective function\n",
    "study = optuna.create_study(\n",
    "    storage=\"sqlite:///db.sqlite3\",\n",
    "    direction='maximize',\n",
    "    study_name='Pet Classification - 1')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learn.fine_tune(1)\n",
    "learn.path = Path('.')\n",
    "learn.export()\n",
    "\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()\n",
    "\n",
    "interp.plot_top_losses(50, nrows=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
